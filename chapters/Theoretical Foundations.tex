\chapter{Theoretical Foundations}
In this chapter, the theoretical Foundations of deep learning model will be introduced. 
\section{Basis of deep learning}
\subsection{Basic concept}
    In \cite{Goodfellow-et-al-2016}, the concept of deep learning is well described. The deep learning is actually a solution that allows 
    computer to learn from the experiences and understand the world. The entire world consists of hierarchical concepts and relations between 
    each concept with the simpler concept in the next level are also included. This concept of structural hierarchy allows computers to learn 
    from some simple concepts to deeper concepts to solve complex problems, see \autoref{Computer learns from the hierarchical structure of concepts}.
    \begin{figure}
		\centering
		\includegraphics[width=0.9\linewidth]{example_images/ConceptDL}
		\caption{Computer learns from the hierarchical structure of concepts}
		\label{Computer learns from the hierarchical structure of concepts}
	  \end{figure}
    The more layers the structure has, the deeper concepts the computer can learn. After the learning, computers can predict phenomenon and make decisions 
    based on what they have learned. The high-level schematic could be summarized in \autoref{The high-level schematic of deep learning}. The phenomenon or object 
    of interests is the input of the deep learning model. Some simple features will be extracted through sensors, for example the image of the wire harness is extracted 
    through RGB-Camera. The following deep neural network(DNN) can extract more abstract features from the simple features and the computer learns the relations between 
    the features after different layers. After the DNN, the further extracted abstract features should be mapped to the output.  
    \begin{figure}
      \centering
      \includegraphics[width=0.6\linewidth]{example_images/DeepLearningSchematic}
      \caption{The high-level schematic of deep learning}
      \label{The high-level schematic of deep learning}
    \end{figure}
  \subsection{Dense neural networks}
  The basic principles of how deep learning works were clarified in the previous section. The next questions are: what is the layer? How does it work to extract features?\\
  In order to answer these two questions more intuitively, the simplest neural network architecture, Dense Neural Network (DNN)\cite{inproceedings}, should be introduced firstly. 
  The DNN simulates the activities of the human brain, which is composed of multiple layers. There could be many neurons in each layer, and all of them are fully connected to 
  all the neurons in the previous layer, see \autoref{Dense neural network}
  The input of the first dense layer is defined as $x_{0}$ and it is also the simple input feature of the network. And the next extracted abstract features are defined as $x_{l}$, 
  where l presents the level of the layer, see \autoref{Mathematical explanation of the feature extraction by layers}.
  The layers are actually defined as some first-order linear mathematical expressions $L _{l}$ with the nonlinear activation function $\Phi_{l}$ in \autoref{eq:layer}, 
  where $w$ is the weight and $b$ is the bias. The nonlinear activation function is important. Without it, the output could be just represented from input by one single layer linearly,
  which is not enough to learn the most of the concepts in the world. Some activation functions are explained in \cite{0706bf17a845490688ef4d7d19df65ba}, e.g., ReLU in \autoref{eq:ReLU}, 
  Sigmoid in \autoref{eq:Sigmoid}, Softmax in \autoref{eq:softmax}. The final step of Schematic, Mapping, can actually be regarded as a layer, which has different options depending on 
  the specific task. For example, for classification, the output layer could be softmax, because it maps the features to probabilities. 
  \begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{example_images/DNN}
    \caption{Dense neural network}
    \label{Dense neural network}
  \end{figure}
  \begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{example_images/mathLayers}
    \caption{Mathematical explanation of the feature extraction by layers}
    \label{Mathematical explanation of the feature extraction by layers}
  \end{figure}
  \begin{align}
    x_{l}=\Phi _{l}(w_{l} \cdot x_{l-1}+ b_{l-1})
     \label{eq:layer}
  \end{align}

  \begin{align}
    \Phi(x) = \begin{cases}
      x& \text{ if } x\ge 0 \\
      0& \text{ else } 
    \end{cases}
     \label{eq:ReLU}
  \end{align}

  \begin{align}
    \Phi(x) = \frac{1}{1+exp(x)^{-1}}  
     \label{eq:Sigmoid}
  \end{align}

  \begin{align}
    \Phi(x_{i}) = \frac{exp(x_{i})}{\sum_{j}exp(x_{j}) } 
     \label{eq:softmax}
  \end{align}
  \subsection{Optimization through backpropagation}
  So far, the process of how initial simple features are extracted and transformed step by step into deeper features through different layers and finally mapped into outputs, 
  i.e., predictions, has been well explained. Then a further question follows: how do networks make accurate predictions?\\
  The transformations and mapping are basically performed by \autoref{eq:layer}. Its result is determined by the activation function $\Phi$, weights $w$ and bias $b$. 
  A GroundTruth $y^{\star }$ needs to be artificially introduced to guide the network to learn the correct behavior, i.e. to predict the desired output. This is explained as
  learning from examples in \cite{russel2010}, which is also well-known as supervised learning. \\
  The prediction of the model or the output after mapping is defined as $y$. A Loss function could be derived. y is actually the result of Mapping, which is actually the output 
  of activation function $\Phi(x)$. It can be further backpropagated onto the feutures from last layer. The optimal weight $w^{star}$ and bais $b^{star}$ are obtained by 
  minimizing the loss equation, see \autoref{eq:Optimization by loss function}. 
  \begin{align}
    w_{l-1}^{\star}, b_{l-1}^{\star}=arg\min_{w_{l-1},b_{l-1}}(\Phi_{l}(\Phi_{l-1}(w_{l-1}\cdot x_{l-1}+b_{l-1}))-y^{\star } ) 
     \label{eq:Optimization by loss function}
  \end{align}
  This approach of optimization can be backwards until the weights and bias of all Layers in the network have been optimized and updated. The whole process is illustrated in 
  \autoref{Parameters are optimized by backpropagation}.
  \begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{example_images/bachpropagation}
    \caption{Parameters are optimized by backpropagation}
    \label{Parameters are optimized by backpropagation}
  \end{figure}
  \subsection{Loss Function}
  The loss function in deep learning describes the difference between the groundtruth and the predicted output. The model parameters are optimized and updated by minimizing 
  this function and backpropagating. The loss function could be minimized because it has a gradient $G$. In \autoref{The loss decreases along the direction of the gradient},
  $\theta$ is the model parameter, such as weight, bias. If the gradient is so small, then the loss is difficult to converge. Or if it is so big, then the loss changes oscillatory.
  Hence, a proper loss function is important for deep learning. 
  \begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{example_images/loss function}
    \caption{The loss decreases along the direction of the gradient $G$}
    \label{The loss decreases along the direction of the gradient}
  \end{figure}
  For example, l1 in \autoref{eq:l1 loss} and l2-loss function in \autoref{eq:l2 loss} are widely used for regression tasks.
  \begin{align}
    l_1=\sum_{i=1}^{n} \left | y-y^{\star} \right |  
     \label{eq:l1 loss}
  \end{align}
  \begin{align}
    l_2=\sum_{i=1}^{n}(y-y^{\star})^2
     \label{eq:l2 loss}
  \end{align}
  In contrast to the l1-loss function, which is not differentiable at zero, l2 is more often chosen as the loss function because the l2 loss equation is differentiable everywhere.
  But there is also a drawback of l2-loss. When there is an unusual data, the l2 loss will become huge due to squaring. It will affect the training. So the choice of loss function 
  needs to be considered depending on different situations, and sometimes it is necessary to test the effect of different loss functions by doing experiments. In \cite{Sun_2018_ECCV},
  they experimented both l1 and l2-loss for the joint regression task and found that l1-loss function worked consistently better than l2-loss function.\\
  Some other loss functions for segmentation tasks are also introduced in \cite{azad2023loss}. Cross Entropy(CE) loss, in \autoref{eq:CE loss}, is used to the difference between two
  probability distributions. $y^{\star}$ is a one-hot encoded vector and only the targets will be considered.
  \begin{align}
    L_{CE}(y,t) = -\sum_{i=1}^{n} y^{\star} \cdot log(y_{i})
     \label{eq:CE loss}
  \end{align}
  \subsection{Hyperparameter}
  In contrast to the model parameters, hyperparameters cannot be optimized at the backpropagation, but they affect the performance of the model in the meantime as well. 
  In \cite{4e568dfccc734aa6a8184f781bac6353}, the hyperparameters are divided into two groups: (1) performance-oriented parameters, which could influence the system's processing
  throughout, such as the number of threads working simultaneously, i.e., number of workers. (2) accuracy-oriented parameters, which could influence the accuracy of the training, e.g.,
  learning rate and drop out probabilities. There is an overlap between these two groups, and some parameters will affect them both, such as the batch size.\\
  Proper hyperparameters are very important for the training, which significantly affects the accuracy and system performance of the model. And tuning hyperparameters requires experience 
  and patience from researchers, because there might be trade-off between some parameters, for example in \autoref{Trade-off between batch size}, When the batch size increases, the 
  accuracy of the model increases and the system performance decreases.
  \begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{example_images/batchSize}
    \caption{Trade-off between batch size}
    \label{Trade-off between batch size}
  \end{figure}
  \subsection{Performance metrics}
  Metrics in deep learning are used to measure the performance of a model. Different tasks require different metrics for evaluating. In this subsection, the commonly used metrics 
  will be presented depending on the different specific task.
  \subsubsection{Classification}
  Two states $H_1$ and $H_2$ are going to be classified, $H_1$ represents the positive state and  $H_2$ represents the negative state. The predicted state is defined as $\hat{H}$.
  Then there are four basic metrics to evaluate the model performance, which are True Positive(TP), False Positive(FP), False Positive(FP) and False Negative(FN).
  The cases they correspond to are shown in \autoref{Four basic metrics for classification}.
  \begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{example_images/basicMetrics}
    \caption{Four basic metrics for classification}
    \label{Four basic metrics for classification}
  \end{figure}
  Then three metrics that are widely used in classification tasks can be derived: \textbf{Accuracy} in \autoref{eq:Accuracy} means the percentage of correctly classified data
  out of the total number of data. \textbf{Precision} in \autoref{eq:Precision} means the percentage of positive data out of the all data predicted to be positive. 
  \textbf{Recall} in \autoref{eq:Recall} means the percentage of correctly predicted positive data out of the all actual positive data.
  \begin{equation}
    \begin{aligned}
      Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
        \label{eq:Accuracy}
    \end{aligned}
  \end{equation}
  \begin{equation}
    \begin{aligned}
      Precision = \frac{TP}{TP + FP}
        \label{eq:Precision}
    \end{aligned}
  \end{equation}
  \begin{equation}
    \begin{aligned}
      Recall = \frac{TP}{TP + FN}
        \label{eq:Recall}
    \end{aligned}
  \end{equation}
\subsubsection{Regression}
The Metrics for regression tasks are similar to its loss functions. For example, Mean Squared Error(MSE) in \autoref{eq:MSEmetrics} and Mean Absolute Error(MAE)
in \autoref{eq:MAEmetrics} could either be used as the loss function as the metrics for evaluation. 
\begin{equation}
  \begin{aligned}
    MSE = \frac{1}{n} \sum_{i=1}^n (\hat{y}_i - y_i)^2
      \label{eq:MSEmetrics}
  \end{aligned}
\end{equation}
\begin{equation}
  \begin{aligned}
    MAE = \frac{1}{n} \sum_{i=1}^n |\hat{y}_i - y_i|
      \label{eq:MAEmetrics}
  \end{aligned}
\end{equation}
\subsubsection{Keypoint detection}
Object Keypoint Similarity (OKS) is a metric used to evaluate the accuracy of keypoint detection. In \cite{Maji_2022_CVPR}, the author introduced it as \autoref{eq:OKS},
which is for each keypoint individually.
\begin{equation}
  \begin{aligned}
    OKS = \frac{\sum_{i=1}^{K} \exp \left( -\frac{d_i^2}{2s^2k_i^2} \right) \cdot \delta(v_i > 0)}{\sum_{i=1}^{K} \delta(v_i > 0)}
      \label{eq:OKS}
  \end{aligned}
\end{equation}
Where:
\begin{itemize}
    \item \( d_i \) is the Euclidean distance between the predicted keypoint and the ground truth keypoint.
    \item \( s \) is scale of the object, e.g., the size of the bounding box.
    \item \( k_i \) is the keypoint specific weight.
    \item \( v_i \) is the visibility flag, e.g., 0 for not labeled and not visible keypoints, 1 for labeled but not visible keypoints, 2 for labeled and visible keypoints.
\end{itemize}
\section{Convolutional neural network}
  DNN can extract very deep features by connecting multiple layers. But Yann LeCun in \cite{LeCun1995-LECCNF} also pointed out some drawbacks:
  \begin{itemize}
    \item [1)] The number of parameters is huge and grows quadratically with the number of neurons per layer, which could lead to the problems like overfitting and memories.
    \item [2)] The topology of the input is entirely ignored and it is not suitable to learn global features due to the full connections of neurons. But the image 
    has a strong 2D local structure. Therefore, images are also not suitable as inputs to DNN.
  \end{itemize}
  Convolutional neural network(CNN) doesn't suffer from either of these problems because of the  weight sharing technique and local connectivity. 
  The input is defined as a 2D-image with $C_{l-1}$ channels, length $H_{l-1}$ and width $W_{l-1}$. The convolutional layer is a four dimensional tensor $C_{l-1} \times C_{l} \times K_{l} \times K_{l}$,
  which means it has $C_{l}$ kernels and each kernel has $C_{l-1}$ filters of size $K_{l} \times K_{l}$. Each channel of the input corresponds to a filter of the kernel. All channels corresponding to 
  the sliding window are mapped into a new channel for the output through all filters of a kernel, see \autoref{Convolutional neural network}. The relationship between the size of the output and 
  the size of the input is shown in \autoref{eq:CNN size change}. When extract the deep features of images, the size of the image will be compressed and the number of channels will increase.
  \begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{example_images/CNN}
    \caption{Convolutional neural network}
    \label{Convolutional neural network}
  \end{figure}
  \begin{equation}
  \begin{aligned}
    H_{l}&=H_{l-1}-K_{l}+1\\
    B_{l}&=B_{l-1}-K_{l}+1
      \label{eq:CNN size change}
  \end{aligned}
  \end{equation}
  It is also possible to add modifications to the convolution, such as padding $P$, stride $S$, dialation $D$. When all modifications are included, the size of output image is in \autoref{eq:Modifications} 
  \begin{equation}
  \begin{aligned}
    H_{l}&=\frac{H_{l-1}+2P-(k_{l}-1)D-1}{S}\\
    W_{l}&=\frac{W_{l-1}+2P-(k_{l}-1)D-1}{S}
      \label{eq:Modifications}
  \end{aligned}
  \end{equation}
\section{Transformer}
  In this section, the transformer will be introduced in detail. Think about keypoint detection tasks for the wire harness. The background is regarded as 
  the disturbance. The basic idea is ignoring the non-relevant part and only focusing on the wire harness. 
  The original model architecture of the transformer is shown in \autoref{The Transformer model architecture}, which is firstly proposed by Vaswani in 2017 \cite{NIPS2017_3f5ee243}. 
  It consists of an encoder and decoder structure. 
  It was initially employed to process natural language processing (NLP) tasks such as text translation and generation. 
\subsection{Attention}
  The input to encoder composes of three parts:
  query(Q), key(K) and value(V). The query can be viewed as the vector form of a word in text. It could correspond to several keys, which represent vectors of the all words 
  in a text. The value represents the extracted information vector from the all words. $Attention(Q, K, v)$ represents the match between each word
  is usually calculated in \autoref{eq:Calculate attention}, where $d_k$ is the dimension of the key vector for avoiding a too large numerator. 
  \begin{equation}
    \begin{aligned}
      Attention(Q,K)=softmax(\frac{Q\cdot K^T}{\sqrt{d_{k}}}) \cdot V 
        \label{eq:Calculate attention}
    \end{aligned}
  \end{equation}
  For example, the score between the words in the sentence "I am a student" are shown in \autoref{Attentions in a sentence}. The four words on a column represent Q, and they each have 4 
  keys. The values in the table grid represent the matches between words. The highest values appear between identical words. The less related the words are to each other, the lower the 
  value. After getting the values between words, it passes through a softmax function, which transforms the values into probabilities between zero and one. Then calculate the dot product
  between the probabilities matrix and the vector V to get the output representation of each word. For easier understanding, the calculation can be expressed as \autoref{Calculate the attentions}.
  \begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{example_images/IamAStudent}
    \caption{Match between words}
    \label{Attentions in a sentence}
  \end{figure}
  \begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.6\linewidth]{example_images/AttenCalculation}
    \caption{Calculate the attentions}
    \label{Calculate the attentions}
  \end{figure}
  \begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.6\linewidth]{example_images/transformer}
    \caption{The Transformer model architecture\cite{NIPS2017_3f5ee243}}
    \label{The Transformer model architecture}
  \end{figure}
\subsection{Position Embedding}
  Transformer is unable to understand the sequential relations of  words in the input. So, it is necessary for the model to understand the position of the elements in the sequence.
  The researchers proposed the Position Embedding approach to realize it, which is presented in \autoref{eq:Position Embedding}, where $pos$ is the position of the word in a sentence,
  $d_{model}$ is the dimension of this word vector and $i$ is the indices in the dimension.  After the Position Embedding is obtained, the word vectors are added with it, which are unique
  according to their position information.
  \begin{equation}
    \begin{aligned}
      PE(pos,i) &= sin(\frac{pos}{10000^{2i/d_{model}}})\\
      PE(pos,i+1) &= cos(\frac{pos}{10000^{2i/d_{model}}})
        \label{eq:Position Embedding}
    \end{aligned}
  \end{equation}
\subsection{Mask in decoder}
  The output of encoder will flow into decoder. The purpose of the decoder is to make the model consider the previously generated results when it generates the target sequence.
  The structure of decoder is similar to the encoder. There are two main blocks in decoder. The first block calculates the attention of the generated output and the second 
  block calculates the attention of the encoder's output with the output of the first block in decoder. While in encoder the attentions of the query with each of its keys will be
  calculated and considered, each word of the target sequence should only consider the word before it. The mask, which will be introduced in this subsection, is included to avoid 
  the information leakage. It is actually an upper triangular matrix, as shown in \autoref{Mask in decoder}. It is added with the score matrix of output and the results is shown
  in \autoref{The score matrix added with mask}. Due to the infinite negative score in the added matrix, after the softmax function the possibilities will be zero. In this case,
  the output words are independent on the future words.
  \begin{figure}[htbp]
		\centering
		\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{example_images/mask}
			\caption{Mask in decoder}
			\label{Mask in decoder}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.45\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{example_images/mask1}
			\caption{The score matrix added with mask}
			\label{The score matrix added with mask}
		\end{subfigure}
		\caption{Use mask to prevent the influence from future words}
		\label{Use mask to prevent the influence from future words}
	\end{figure}
\subsection{Vision Transformer}
  With the success of Transformer in the field of NLP, researchers began to explore its application in computer vision. The input image of size $C\times H\times W$, where 
  $C$ corresponds to the channel, $H$ corresponds to the height of the image and $W$ corresponds to the width of the figure, is divided into small patches of size$C\times P \times P$.
  A learnable embedding for classification $x_{class}$ and position embeddings will be added to the patch embeddings and fed to the encoder of the Transformer. After going through
  multiple layers of Transformer encoder, the output sequence contains global information about the image. $x_class$ is classified by a head, which is implemented by the 
  Multilayer Perceptron(MLP). The process is shown in \autoref{fig:Vision Transformer}. With different heads, Vision Transformer(ViT) can also be utilized for different tasks 
  such as object detection, segmentation, keypoints detection.
  \begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{example_figures/VisionTransformer.png}
    \caption{Vision Transformer \cite{dosovitskiy2021an}}
    \label{fig:Vision Transformer}
  \end{figure}
\section{Full-topology and individual-segment detection}
 The topology of a eleven-segment wire harness is shown in \autoref{fig:Topology of the eleven-segment wire harness}. It could be either detected as a single
 object, or divided into individual segments and each of them is regarded as an individual segment to be detected. If it is detected as a single object, 
 the dataset should be annotated in a fixed order of segments if there is no algorithm of segments sorting, e.g., from $s_1$ to $s_10$. 
 We define two sets $S$ and $\hat{S}$ of dimension $S \times K \times H \times W$, which contain the predicted- and ground-truth segments of the wire harness. 
 Each segment of dimension $K\times H\times W$ contains all the keypoints on it. After the model makes a prediction, the predicted tensor and ground truth may 
 not match due to the problem that some images of wire harness is not labeled according to the topology. This could lead to the failure of training if we do not 
 sort the order of the segments. Hence we adjust the order through Algorithm \ref{Segments Sorting} before the model calculates the loss at each forward propagation. 
 The segments in $\hat{S}$ will be computed against all possible matches in $S$ to obtain the minimum binary cross-entropy loss. Once the minimum loss is determined, 
 its corresponding segment is excluded from $S$ and stored to $S\star$. The next round of matching begins until the best match is found for all segments in $\hat{S}$. 
 $S^{star}$ is the new set obtained by sorting the prediction set $S$ and matched with the ground-truth set $\hat{S}$.
 \begin{algorithm} 
  \caption{Segments Sorting}
  \label{Segments Sorting}
  \begin{algorithmic}
  \STATE \textbf{Input:} $S, \hat{S}$
  \STATE \textbf{Output:} $S^{\star}$
  \STATE $ L\leftarrow \left [\ \right ] $
  \STATE $ S^{\star}\leftarrow \left [\ \right ] $
  \FOR{$\hat{B} \in \hat{S}$}
      \FOR{$B \in S$}
          \STATE $L.append(GetBinaryCrossEntropyLoss(\hat{B},B))$
      \ENDFOR
      \STATE $S^{\star}.append({argmin(L)})$
      \STATE $S.pop({argmin(L)})$
  \ENDFOR
  \RETURN $S^{\star}$
  \end{algorithmic}
\end{algorithm}

 \begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.5\linewidth]{example_figures/Topology.pdf}
  \caption{Validation}
  \label{fig:Topology of the eleven-segment wire harness}
\end{figure}
\section{Object detection}
  Object detection is a computer vision task that aims to recognize and localize objects in the image. Object detection are mainly divided into two classes: 
  One Stage(YOLO family) and Two Stage detectors(RCNN family). The basic theories of these methods are described in this section.\\
  Take the well-known Faster RCNN method as an example of a typical two-stage object detection\cite{NIPS2015_14bfa6bb}. It is composed of two modules: the 
  first module is a Region Proposal Network, which proposes the region includes the object. The second module is actually a Fast RCNN detector, which employes
  the proposals to classify objects. The architecture is shown in \autoref{Two stage object detection}. The convolutional layers extract the features of the 
  input image to generate a feature map. A small network will slide the whole feature map and its input is the $n \times n$ spatial window of the extracted feature map.
  The sliding window is processed twice separately: The first process is to predict the possibility for classification and the second is to bound the object for localization.\\
  \begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.6\linewidth]{example_images/two_stage_detection}
    \caption{Two stage object detection\cite{NIPS2015_14bfa6bb}}
    \label{Two stage object detection}
  \end{figure}
  \begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{example_images/Region Proposal}
    \caption{How the sliding window works}
    \label{How the sliding window works}
  \end{figure}
  Compared with two-stage detection, which generates proposals firstly and in the next stage for the regression and classification. The one-stage detection method 
  predicts on the input image directly and train the model end to end. This type of model is represented by YOLO(You Only Look Once), which has a high speed of 
  prediction and is suitable for real-time detection. The basic idea of the one-stage detection method is shown in \autoref{One-stage object detection}.
  \begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{example_figures/TwoStageDetection}
    \caption{One stage object detection}
    \label{One stage object detection}
  \end{figure}
\section{Keypoints detection}
  Object detection localizes and classifies the target object, while keypoints detection focuses on some specific keypoints on the obeject. There are two common methods
  in deep learning for keypoints detection: Heatmap-based method and coordinate regression-based method.\\
  For Heatmap-based method, the model takes the image as input and outputs the heatmaps of size $H\times W$ for each keypoint. The values in the heatmap represent the possibilities,
  that the keypoint is localized in this position. Some examples of the generated heatmap are shown in \autoref{Examples of the heatmaps}. The lighter the color, the higher the 
  probability that the keypoint is located. The position could be obtained by finding the highest possibility as \autoref{eq:maximum likelihood}.\\
  \begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.6\linewidth]{example_images/heatmap_segment}
    \caption{Examples of the heatmaps}
    \label{Examples of the heatmaps}
  \end{figure}
  The model is trained end-to-end when the keypoints are derived by the coordinate regression-based method. It tasks the image as input and outputs the position of each 
  keypoint represented by a numerical value in the $x$ and $y$ directions. As it doesn't need the process of generating heatmaps and calculating heatmaps, the training speed is 
  higher than the heatmap-based method. 
\section{Transfer Learning}
  The purpose of transfer learning is to apply knowledge from a model that has been trained on one task to another related task. It's suitable for special tasks where only 
  small data sets are available. The model is pre-trained with some easily obtainable dataset\cite{weiss-2016-transferLearning}, e.g., COCO\cite{lin-2014-microsoftCOCO},
  ImageNet\cite{ILSVRC15} and fein-tuned on the task-specific dataset. The pre-training could also be performed on synthetic dataset, which is generated by computer.
  The synthetic dataset is similar to the real dataset and annotated automatically. The common methods of generating synthetic datasets include CAD-modeling and rendering.
  
  